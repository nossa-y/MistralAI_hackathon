{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uz5WsGW9dUGj",
    "outputId": "85ab428b-a586-4173-a242-63e6aa2c7372"
   },
   "outputs": [],
   "source": [
    "%pip install -qU mistralai langchain-core langchain langchain-mistralai gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LejWEzjdWIdN",
    "outputId": "2ec70e77-245e-4dde-db64-166e5654a0c4"
   },
   "outputs": [],
   "source": [
    "import getpass, os\n",
    "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Mistral AI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "iPXCRP_4Vz6B",
    "outputId": "b79feb71-fc7c-4314-9e35-3227aa182bdd"
   },
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Glass()) as demo:\n",
    "    progress_textbox = gr.components.Textbox(interactive=False, visible=True, autoscroll=True, show_label=False)\n",
    "    empty_html_template = \"\"\"\n",
    "    <div style='height: 20px; width: 100%; background-color: pink;'>{content}</div>\n",
    "    \"\"\"\n",
    "    html_area = gr.HTML(empty_html_template.format(content=\"\"))\n",
    "    chatbot = gr.Chatbot(show_label=False)\n",
    "    msg = gr.Textbox(show_label=False)\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    statuses = [\n",
    "        \"step I success\",\n",
    "        \"step II success\",\n",
    "        \"step III failure\",\n",
    "        \"step III success\",\n",
    "        \"step IV success\",\n",
    "        \"step V success\"\n",
    "    ]\n",
    "\n",
    "    def process():\n",
    "        for i in range(len(statuses)+1):\n",
    "            time.sleep(0.5)\n",
    "            yield '\\n'.join(statuses[0:i])\n",
    "\n",
    "    def html_log():\n",
    "        for i in range(len(statuses)+1):\n",
    "            time.sleep(0.5)\n",
    "            yield empty_html_template.format(content='<br />'.join(statuses[0:i]))\n",
    "\n",
    "    def bot(history):\n",
    "        history_langchain_format = []\n",
    "\n",
    "        # Convert the history to the format expected by the ChatMistralAI client\n",
    "        for entry in history:\n",
    "            if len(entry) == 2:\n",
    "                human, ai = entry\n",
    "                history_langchain_format.append(ChatMessage(role=\"user\", content=human))\n",
    "                if ai is not None:\n",
    "                    history_langchain_format.append(ChatMessage(role=\"assistant\", content=ai))\n",
    "\n",
    "        from mistralai.client import MistralClient\n",
    "        client = MistralClient()\n",
    "        response = client.chat_stream(\n",
    "            model=model,\n",
    "            messages=history_langchain_format,\n",
    "        )\n",
    "\n",
    "        history[-1][1] = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                history[-1][1] += chunk.choices[0].delta.content\n",
    "                yield history\n",
    "\n",
    "    action_step = msg.submit(\n",
    "            user, [msg, chatbot], [msg, chatbot], queue=False\n",
    "        ).then(\n",
    "          # clear progress_textbox\n",
    "          lambda: None, None, progress_textbox, queue=False\n",
    "        ).then(\n",
    "          # clear html_area\n",
    "          lambda: empty_html_template.format(content=\"\"), None, html_area, queue=False\n",
    "        ).then(\n",
    "            process, None, progress_textbox\n",
    "        ).then(\n",
    "            html_log, None, html_area\n",
    "        )\n",
    "\n",
    "\n",
    "    # final response to user query\n",
    "    action_step.then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "\n",
    "    # clear\n",
    "    clear.click(lambda: None, None, chatbot, queue=False).then(\n",
    "       # clear progress_textbox\n",
    "       lambda: None, None, progress_textbox, queue=False\n",
    "    ).then(\n",
    "       # clear html_area\n",
    "       lambda: empty_html_template.format(content=\"\"), None, html_area, queue=False\n",
    "    )\n",
    "\n",
    "demo.queue()\n",
    "demo.launch(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
